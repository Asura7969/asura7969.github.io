<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Spark Shuffle | Asura7969 Blog</title><meta name="keywords" content="spark"><meta name="author" content="Asura7969"><meta name="copyright" content="Asura7969"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Spark Shuffle简介初始化SparkContext时候会createSparkEnv,创建ShuffleManager 12345678910...&#x2F;&#x2F; Let the user specify short names for shuffle managersval shortShuffleMgrNames &#x3D; Map(  &quot;sort&quot; -&gt; classOf[o">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark Shuffle">
<meta property="og:url" content="https://asura7969.github.io/2020/10/26/Spark%20Shuffle/index.html">
<meta property="og:site_name" content="Asura7969 Blog">
<meta property="og:description" content="Spark Shuffle简介初始化SparkContext时候会createSparkEnv,创建ShuffleManager 12345678910...&#x2F;&#x2F; Let the user specify short names for shuffle managersval shortShuffleMgrNames &#x3D; Map(  &quot;sort&quot; -&gt; classOf[o">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://asura7969.github.io/img/topimg/202105161054.png">
<meta property="article:published_time" content="2020-10-26T00:31:57.000Z">
<meta property="article:modified_time" content="2021-05-17T14:36:44.381Z">
<meta property="article:author" content="Asura7969">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://asura7969.github.io/img/topimg/202105161054.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://asura7969.github.io/2020/10/26/Spark%20Shuffle/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark Shuffle',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-05-17 22:36:44'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/shanyi.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">38</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/topimg/202105161054.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Asura7969 Blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark Shuffle</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-10-26T00:31:57.000Z" title="发表于 2020-10-26 08:31:57">2020-10-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-05-17T14:36:44.381Z" title="更新于 2021-05-17 22:36:44">2021-05-17</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/shuffle/">shuffle</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark Shuffle"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Spark-Shuffle"><a href="#Spark-Shuffle" class="headerlink" title="Spark Shuffle"></a>Spark Shuffle</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>初始化<code>SparkContext</code>时候会createSparkEnv,创建<code>ShuffleManager</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="comment">// Let the user specify short names for shuffle managers</span></span><br><span class="line"><span class="keyword">val</span> shortShuffleMgrNames = <span class="type">Map</span>(</span><br><span class="line">  <span class="string">&quot;sort&quot;</span> -&gt; classOf[org.apache.spark.shuffle.sort.<span class="type">SortShuffleManager</span>].getName,</span><br><span class="line">  <span class="string">&quot;tungsten-sort&quot;</span> -&gt; classOf[org.apache.spark.shuffle.sort.<span class="type">SortShuffleManager</span>].getName)</span><br><span class="line"><span class="keyword">val</span> shuffleMgrName = conf.get(<span class="string">&quot;spark.shuffle.manager&quot;</span>, <span class="string">&quot;sort&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> shuffleMgrClass =</span><br><span class="line">  shortShuffleMgrNames.getOrElse(shuffleMgrName.toLowerCase(<span class="type">Locale</span>.<span class="type">ROOT</span>), shuffleMgrName)</span><br><span class="line"><span class="keyword">val</span> shuffleManager = instantiateClass[<span class="type">ShuffleManager</span>](shuffleMgrClass)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h2 id="org-apache-spark-shuffle-sort-SortShuffleManager"><a href="#org-apache-spark-shuffle-sort-SortShuffleManager" class="headerlink" title="org.apache.spark.shuffle.sort.SortShuffleManager"></a>org.apache.spark.shuffle.sort.SortShuffleManager</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">SortShuffleManager</span>(<span class="params">conf: <span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">ShuffleManager</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line">  ...</span><br><span class="line">  <span class="comment">// 初始化 BlockManager</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">lazy</span> <span class="keyword">val</span> shuffleExecutorComponents = loadShuffleExecutorComponents(conf)</span><br><span class="line">  <span class="comment">// 初始化 IndexShuffleBlockResolver</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">val</span> shuffleBlockResolver = <span class="keyword">new</span> <span class="type">IndexShuffleBlockResolver</span>(conf)</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 注册ShuffleHandle</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">registerShuffle</span></span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">      shuffleId: <span class="type">Int</span>,</span><br><span class="line">      dependency: <span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]): <span class="type">ShuffleHandle</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">SortShuffleWriter</span>.shouldBypassMergeSort(conf, dependency)) &#123;</span><br><span class="line">      <span class="comment">// map端不需要聚合 &amp;&amp; 分区数 &lt;= spark.shuffle.sort.bypassMergeThreshold(default = 200)</span></span><br><span class="line">      <span class="comment">// 直接生成numPartitions文件，然后在最后将它们连接起来</span></span><br><span class="line">      <span class="comment">// 这样可以避免执行两次序列化和反序列化以将溢出的文件合并在一起，而正常的代码路径会发生这种情况。</span></span><br><span class="line">      <span class="comment">// 缺点是一次打开多个文件，因此分配给缓冲区的内存更多。</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">        shuffleId, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="type">SortShuffleManager</span>.canUseSerializedShuffle(dependency)) &#123;</span><br><span class="line">      <span class="comment">// Otherwise, try to buffer map outputs in a serialized form, since this is more efficient:</span></span><br><span class="line">      <span class="comment">// 1、map端不需要聚合</span></span><br><span class="line">      <span class="comment">// 2、设置的序列化方式支持relocation，Serializer可以对已经序列化的对象进行排序（默认的KryoSerializer和JavaSerializer都支持）</span></span><br><span class="line">      <span class="comment">// 3、分区数小于1600万</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">        shuffleId, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Otherwise, buffer map outputs in a deserialized form:</span></span><br><span class="line">      <span class="comment">// 否则, 以反序列化形式shuffle</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">BaseShuffleHandle</span>(shuffleId, dependency)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 获取reduce</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getReader</span></span>[<span class="type">K</span>, <span class="type">C</span>](</span><br><span class="line">      handle: <span class="type">ShuffleHandle</span>,</span><br><span class="line">      startMapIndex: <span class="type">Int</span>,</span><br><span class="line">      endMapIndex: <span class="type">Int</span>,</span><br><span class="line">      startPartition: <span class="type">Int</span>,</span><br><span class="line">      endPartition: <span class="type">Int</span>,</span><br><span class="line">      context: <span class="type">TaskContext</span>,</span><br><span class="line">      metrics: <span class="type">ShuffleReadMetricsReporter</span>): <span class="type">ShuffleReader</span>[<span class="type">K</span>, <span class="type">C</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> blocksByAddress = <span class="type">SparkEnv</span>.get.mapOutputTracker.getMapSizesByExecutorId(</span><br><span class="line">      handle.shuffleId, startMapIndex, endMapIndex, startPartition, endPartition)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BlockStoreShuffleReader</span>(</span><br><span class="line">      handle.asInstanceOf[<span class="type">BaseShuffleHandle</span>[<span class="type">K</span>, _, <span class="type">C</span>]], blocksByAddress, context, metrics,</span><br><span class="line">      shouldBatchFetch = canUseBatchFetch(startPartition, endPartition, context))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 获取map</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getWriter</span></span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">      handle: <span class="type">ShuffleHandle</span>,</span><br><span class="line">      mapId: <span class="type">Long</span>,</span><br><span class="line">      context: <span class="type">TaskContext</span>,</span><br><span class="line">      metrics: <span class="type">ShuffleWriteMetricsReporter</span>): <span class="type">ShuffleWriter</span>[<span class="type">K</span>, <span class="type">V</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> mapTaskIds = taskIdMapsForShuffle.computeIfAbsent(</span><br><span class="line">      handle.shuffleId, _ =&gt; <span class="keyword">new</span> <span class="type">OpenHashSet</span>[<span class="type">Long</span>](<span class="number">16</span>))</span><br><span class="line">    mapTaskIds.synchronized &#123; mapTaskIds.add(context.taskAttemptId()) &#125;</span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">SparkEnv</span>.get</span><br><span class="line">    handle <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> unsafeShuffleHandle: <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>] =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">UnsafeShuffleWriter</span>(</span><br><span class="line">          env.blockManager,</span><br><span class="line">          context.taskMemoryManager(),</span><br><span class="line">          unsafeShuffleHandle,</span><br><span class="line">          mapId,</span><br><span class="line">          context,</span><br><span class="line">          env.conf,</span><br><span class="line">          metrics,</span><br><span class="line">          shuffleExecutorComponents)</span><br><span class="line">      <span class="keyword">case</span> bypassMergeSortHandle: <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>] =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleWriter</span>(</span><br><span class="line">          env.blockManager,</span><br><span class="line">          bypassMergeSortHandle,</span><br><span class="line">          mapId,</span><br><span class="line">          env.conf,</span><br><span class="line">          metrics,</span><br><span class="line">          shuffleExecutorComponents)</span><br><span class="line">      <span class="keyword">case</span> other: <span class="type">BaseShuffleHandle</span>[<span class="type">K</span> <span class="meta">@unchecked</span>, <span class="type">V</span> <span class="meta">@unchecked</span>, _] =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">SortShuffleWriter</span>(</span><br><span class="line">          shuffleBlockResolver, other, mapId, context, shuffleExecutorComponents)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>spark shuffle由三部分组成：<code>Writer</code>、<code>Reader</code>、<code>Resolver</code></p>
<p>driver端初始化<code>ShuffleWriteProcessor</code>,executors端在每个<code>ShuffleMapTask</code>中使用它,调用write方法,<br>从<code>ShuffleManager</code>获取<code>ShuffleWriter</code>（三种Writer）并触发rdd计算,最后返回<code>MapStatus</code>(shuffle的结果信息:block manager地址,shufle文件)</p>
<p><code>Writer实现</code></p>
<ul>
<li><strong>org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter</strong><blockquote>
<p>map端不需要聚合</p>
<p>partition number &lt;= 200(default)</p>
</blockquote>
</li>
</ul>
<p>每个partition输出一个文件,最终map task都结束后会把本节点所有partition的文件合并成一个shuffle文件(分区数不宜过多,&lt;= 200),和一个index文件</p>
<p><em><code>优点</code></em></p>
<ul>
<li>Map创建的文件少</li>
<li>Random IO更少</li>
</ul>
<p><em><code>缺点</code></em></p>
<ul>
<li>排序比Hash慢</li>
<li>一次打开多个文件，因此分配给缓冲区的内存更多</li>
</ul>
<h3 id="org-apache-spark-shuffle-sort-BypassMergeSortShuffleWriter"><a href="#org-apache-spark-shuffle-sort-BypassMergeSortShuffleWriter" class="headerlink" title="org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter"></a>org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"> <span class="meta">@Override</span></span><br><span class="line"> <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(Iterator&lt;Product2&lt;K, V&gt;&gt; records)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">   <span class="keyword">assert</span> (partitionWriters == <span class="keyword">null</span>);</span><br><span class="line">   ShuffleMapOutputWriter mapOutputWriter = shuffleExecutorComponents</span><br><span class="line">       .createMapOutputWriter(shuffleId, mapId, numPartitions);</span><br><span class="line">   <span class="keyword">try</span> &#123;</span><br><span class="line">     <span class="keyword">if</span> (!records.hasNext()) &#123;</span><br><span class="line">       <span class="comment">// 没有数据则输出一个空文件</span></span><br><span class="line">       partitionLengths = mapOutputWriter.commitAllPartitions().getPartitionLengths();</span><br><span class="line">       mapStatus = MapStatus$.MODULE$.apply(</span><br><span class="line">         blockManager.shuffleServerId(), partitionLengths, mapId);</span><br><span class="line">       <span class="keyword">return</span>;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">final</span> SerializerInstance serInstance = serializer.newInstance();</span><br><span class="line">     <span class="keyword">final</span> <span class="keyword">long</span> openStartTime = System.nanoTime();</span><br><span class="line">     partitionWriters = <span class="keyword">new</span> DiskBlockObjectWriter[numPartitions];</span><br><span class="line">     partitionWriterSegments = <span class="keyword">new</span> FileSegment[numPartitions];</span><br><span class="line">     <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">       <span class="keyword">final</span> Tuple2&lt;TempShuffleBlockId, File&gt; tempShuffleBlockIdPlusFile =</span><br><span class="line">           blockManager.diskBlockManager().createTempShuffleBlock();</span><br><span class="line">       <span class="keyword">final</span> File file = tempShuffleBlockIdPlusFile._2();</span><br><span class="line">       <span class="keyword">final</span> BlockId blockId = tempShuffleBlockIdPlusFile._1();</span><br><span class="line">       partitionWriters[i] =</span><br><span class="line">           blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics);</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="comment">// Creating the file to write to and creating a disk writer both involve interacting with</span></span><br><span class="line">     <span class="comment">// the disk, and can take a long time in aggregate when we open many files, so should be</span></span><br><span class="line">     <span class="comment">// included in the shuffle write time.</span></span><br><span class="line">     writeMetrics.incWriteTime(System.nanoTime() - openStartTime);</span><br><span class="line"></span><br><span class="line">     <span class="keyword">while</span> (records.hasNext()) &#123;</span><br><span class="line">       <span class="keyword">final</span> Product2&lt;K, V&gt; record = records.next();</span><br><span class="line">       <span class="keyword">final</span> K key = record._1();</span><br><span class="line">       <span class="comment">// 如果有数据,按key对应的分区,分别写入对应的文件</span></span><br><span class="line">       partitionWriters[partitioner.getPartition(key)].write(key, record._2());</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">       <span class="keyword">try</span> (DiskBlockObjectWriter writer = partitionWriters[i]) &#123;</span><br><span class="line">         partitionWriterSegments[i] = writer.commitAndGet();</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="comment">// 将多个分区文件合并成一个文件,并生成索引文件</span></span><br><span class="line">     partitionLengths = writePartitionedData(mapOutputWriter);</span><br><span class="line">     mapStatus = MapStatus$.MODULE$.apply(</span><br><span class="line">       blockManager.shuffleServerId(), partitionLengths, mapId);</span><br><span class="line">   &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">     <span class="keyword">try</span> &#123;</span><br><span class="line">       mapOutputWriter.abort(e);</span><br><span class="line">     &#125; <span class="keyword">catch</span> (Exception e2) &#123;</span><br><span class="line">       logger.error(<span class="string">&quot;Failed to abort the writer after failing to write map output.&quot;</span>, e2);</span><br><span class="line">       e.addSuppressed(e2);</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">throw</span> e;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Concatenate all of the per-partition files into a single combined file.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@return</span> array of lengths, in bytes, of each partition of the file (used by map output tracker).</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">long</span>[] writePartitionedData(ShuffleMapOutputWriter mapOutputWriter) <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">   <span class="comment">// Track location of the partition starts in the output file</span></span><br><span class="line">   <span class="keyword">if</span> (partitionWriters != <span class="keyword">null</span>) &#123;</span><br><span class="line">     <span class="keyword">final</span> <span class="keyword">long</span> writeStartTime = System.nanoTime();</span><br><span class="line">     <span class="keyword">try</span> &#123;</span><br><span class="line">       <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">         <span class="keyword">final</span> File file = partitionWriterSegments[i].file();</span><br><span class="line">         ShufflePartitionWriter writer = mapOutputWriter.getPartitionWriter(i);</span><br><span class="line">         <span class="keyword">if</span> (file.exists()) &#123;</span><br><span class="line">           <span class="keyword">if</span> (transferToEnabled) &#123;</span><br><span class="line">             <span class="comment">// Using WritableByteChannelWrapper to make resource closing consistent between</span></span><br><span class="line">             <span class="comment">// this implementation and UnsafeShuffleWriter.</span></span><br><span class="line">             Optional&lt;WritableByteChannelWrapper&gt; maybeOutputChannel = writer.openChannelWrapper();</span><br><span class="line">             <span class="keyword">if</span> (maybeOutputChannel.isPresent()) &#123;</span><br><span class="line">               writePartitionedDataWithChannel(file, maybeOutputChannel.get());</span><br><span class="line">             &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">               writePartitionedDataWithStream(file, writer);</span><br><span class="line">             &#125;</span><br><span class="line">           &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">             writePartitionedDataWithStream(file, writer);</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">if</span> (!file.delete()) &#123;</span><br><span class="line">             logger.error(<span class="string">&quot;Unable to delete file for partition &#123;&#125;&quot;</span>, i);</span><br><span class="line">           &#125;</span><br><span class="line">         &#125;</span><br><span class="line">       &#125;</span><br><span class="line">     &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">       writeMetrics.incWriteTime(System.nanoTime() - writeStartTime);</span><br><span class="line">     &#125;</span><br><span class="line">     partitionWriters = <span class="keyword">null</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="comment">// 生成 index文件</span></span><br><span class="line">   <span class="keyword">return</span> mapOutputWriter.commitAllPartitions();</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p><img src="/img/blog/spark-bypassMergeSortShuffleWriter.png" alt="spark-bypassMergeSortShuffleWriter.png"></p>
<h3 id="org-apache-spark-shuffle-sort-SortShuffleWriter"><a href="#org-apache-spark-shuffle-sort-SortShuffleWriter" class="headerlink" title="org.apache.spark.shuffle.sort.SortShuffleWriter"></a>org.apache.spark.shuffle.sort.SortShuffleWriter</h3><p><img src="/img/blog/SortShuffleWriter.png" alt="SortShuffleWriter.png"></p>
<p>每个map task会现在内存做排序,内存达到阈值,会生spill生成一个小文件,最终map task结束后会对所有小文件做一个类似于多路归并的排序,生成shuffle文件,和一个index文件</p>
<p><code>SortShuffleWriter</code>中排序的实现由<code>ExternalSorter</code>实现,数据spill前存在内存中(<code>org.apache.spark.util.collection.AppendOnlyMap</code>,key与value存在同一数组中)</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 若需要aggregator,使用 map, 否则使用 buffer</span></span><br><span class="line"><span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> map = <span class="keyword">new</span> <span class="type">PartitionedAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line"><span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> buffer = <span class="keyword">new</span> <span class="type">PartitionedPairBuffer</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  sorter = <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">    <span class="comment">// map端需要聚合,选择PartitionedAppendOnlyMap</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">      context, dep.aggregator, <span class="type">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// In this case we pass neither an aggregator nor an ordering to the sorter, because we don&#x27;t</span></span><br><span class="line">    <span class="comment">// care whether the keys get sorted in each partition; that will be done on the reduce side</span></span><br><span class="line">    <span class="comment">// if the operation being run is sortByKey.</span></span><br><span class="line">    <span class="comment">// mao端不需要聚合选择PartitionedPairBuffer</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](</span><br><span class="line">      context, aggregator = <span class="type">None</span>, <span class="type">Some</span>(dep.partitioner), ordering = <span class="type">None</span>, dep.serializer)</span><br><span class="line">  &#125;</span><br><span class="line">  sorter.insertAll(records)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Don&#x27;t bother including the time to open the merged output file in the shuffle write time,</span></span><br><span class="line">  <span class="comment">// because it just opens a single file, so is typically too fast to measure accurately</span></span><br><span class="line">  <span class="comment">// (see SPARK-3570).</span></span><br><span class="line">  <span class="keyword">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class="line">  <span class="keyword">val</span> tmp = <span class="type">Utils</span>.tempFileWith(output)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> blockId = <span class="type">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class="type">IndexShuffleBlockResolver</span>.<span class="type">NOOP_REDUCE_ID</span>)</span><br><span class="line">    <span class="keyword">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)</span><br><span class="line">    <span class="comment">// 生成索引文件</span></span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class="line">    mapStatus = <span class="type">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">      logError(<span class="string">s&quot;Error while deleting temp file <span class="subst">$&#123;tmp.getAbsolutePath&#125;</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>Shuffle 阶段有几次预聚合?数据结构是什么样的?</p>
</blockquote>
<p>如果map端需要预聚合,sorter的实现为PartitionedAppendOnlyMap<br><code>insertAll</code>方法插入键值对时,如果对应的<em>K</em>已存在<em>V</em>,就第一次merge,<br>最后合并文件时,会把所有spill文件 和 内存中的数据组成<em>Iterator</em>,放到一个优先级队列,此优先级队列把每个<em>Iterator</em>的第一个值比较,<br>之后从这个Queue中拿值,判断<em>K</em>是否相等,若相等,则继续合并值</p>
<p><img src="/img/blog/spark-mapShuffle-combine.png" alt="spark-mapShuffle-combine.png"></p>
<p><strong>org.apache.spark.util.collection.ExternalSorter</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insertAll</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> stop combining if we find that the reduction factor isn&#x27;t high</span></span><br><span class="line">  <span class="keyword">val</span> shouldCombine = aggregator.isDefined</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (shouldCombine) &#123;</span><br><span class="line">    <span class="comment">// Combine values in-memory first using our AppendOnlyMap</span></span><br><span class="line">    <span class="keyword">val</span> mergeValue = aggregator.get.mergeValue</span><br><span class="line">    <span class="keyword">val</span> createCombiner = aggregator.get.createCombiner</span><br><span class="line">    <span class="keyword">var</span> kv: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>] = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">val</span> update = (hadValue: <span class="type">Boolean</span>, oldValue: <span class="type">C</span>) =&gt; &#123;</span><br><span class="line">      <span class="keyword">if</span> (hadValue) mergeValue(oldValue, kv._2) <span class="keyword">else</span> createCombiner(kv._2)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (records.hasNext) &#123;</span><br><span class="line">      addElementsRead()</span><br><span class="line">      kv = records.next()</span><br><span class="line">      <span class="comment">// 第一次预聚合</span></span><br><span class="line">      map.changeValue((getPartition(kv._1), kv._1), update)</span><br><span class="line">      <span class="comment">// 判断是否需要spill</span></span><br><span class="line">      maybeSpillCollection(usingMap = <span class="literal">true</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Stick values into our buffer</span></span><br><span class="line">    <span class="keyword">while</span> (records.hasNext) &#123;</span><br><span class="line">      addElementsRead()</span><br><span class="line">      <span class="keyword">val</span> kv = records.next()</span><br><span class="line">      buffer.insert(getPartition(kv._1), kv._1, kv._2.asInstanceOf[<span class="type">C</span>])</span><br><span class="line">      maybeSpillCollection(usingMap = <span class="literal">false</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spill the current in-memory collection to disk if needed.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param usingMap whether we&#x27;re using a map or buffer as our current in-memory collection</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeSpillCollection</span></span>(usingMap: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> estimatedSize = <span class="number">0</span>L</span><br><span class="line">  <span class="keyword">if</span> (usingMap) &#123;</span><br><span class="line">    estimatedSize = map.estimateSize()</span><br><span class="line">    <span class="comment">// maybeSpill方法为spill的实现(org.apache.spark.util.collection.Spillable)</span></span><br><span class="line">    <span class="keyword">if</span> (maybeSpill(map, estimatedSize)) &#123;</span><br><span class="line">      map = <span class="keyword">new</span> <span class="type">PartitionedAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    estimatedSize = buffer.estimateSize()</span><br><span class="line">    <span class="keyword">if</span> (maybeSpill(buffer, estimatedSize)) &#123;</span><br><span class="line">      buffer = <span class="keyword">new</span> <span class="type">PartitionedPairBuffer</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (estimatedSize &gt; _peakMemoryUsedBytes) &#123;</span><br><span class="line">    _peakMemoryUsedBytes = estimatedSize</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spill our in-memory collection to a sorted file that we can merge later.</span></span><br><span class="line"><span class="comment"> * We add this file into `spilledFiles` to find it later.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param collection whichever collection we&#x27;re using (map or buffer)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">override</span> <span class="keyword">protected</span>[<span class="keyword">this</span>] <span class="function"><span class="keyword">def</span> <span class="title">spill</span></span>(collection: <span class="type">WritablePartitionedPairCollection</span>[<span class="type">K</span>, <span class="type">C</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 调用AppendOnlyMap.destructiveSortedWritablePartitionedIterator 进行内存排序</span></span><br><span class="line">  <span class="keyword">val</span> inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class="line">  <span class="keyword">val</span> spillFile = spillMemoryIteratorToDisk(inMemoryIterator)</span><br><span class="line">  spills += spillFile</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Merge-sort a sequence of (K, C) iterators using a given a comparator for the keys.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">mergeSort</span></span>(iterators: <span class="type">Seq</span>[<span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]], comparator: <span class="type">Comparator</span>[<span class="type">K</span>])</span><br><span class="line">    : <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] =&#123;</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">val</span> bufferedIters = iterators.filter(_.hasNext).map(_.buffered)</span><br><span class="line">  <span class="class"><span class="keyword">type</span> <span class="title">Iter</span> </span>= <span class="type">BufferedIterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]</span><br><span class="line">  <span class="keyword">val</span> heap = <span class="keyword">new</span> mutable.<span class="type">PriorityQueue</span>[<span class="type">Iter</span>]()(<span class="keyword">new</span> <span class="type">Ordering</span>[<span class="type">Iter</span>] &#123;</span><br><span class="line">    <span class="comment">// Use the reverse order because PriorityQueue dequeues the max</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(x: <span class="type">Iter</span>, y: <span class="type">Iter</span>): <span class="type">Int</span> = comparator.compare(y.head._1, x.head._1)</span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="comment">// 放到优先级队列</span></span><br><span class="line">  heap.enqueue(bufferedIters: _*)  <span class="comment">// Will contain only the iterators with hasNext = true</span></span><br><span class="line">  <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = !heap.isEmpty</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>] = &#123;</span><br><span class="line">      <span class="keyword">if</span> (!hasNext) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoSuchElementException</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> firstBuf = heap.dequeue()</span><br><span class="line">      <span class="keyword">val</span> firstPair = firstBuf.next()</span><br><span class="line">      <span class="keyword">if</span> (firstBuf.hasNext) &#123;</span><br><span class="line">        <span class="comment">// 取完后放回到队列中</span></span><br><span class="line">        heap.enqueue(firstBuf)</span><br><span class="line">      &#125;</span><br><span class="line">      firstPair</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 第二次预聚合</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">mergeWithAggregation</span></span>(</span><br><span class="line">    iterators: <span class="type">Seq</span>[<span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]],</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    comparator: <span class="type">Comparator</span>[<span class="type">K</span>],</span><br><span class="line">    totalOrder: <span class="type">Boolean</span>)</span><br><span class="line">    : <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] =</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">if</span> (!totalOrder) &#123;</span><br><span class="line">      ...</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="comment">// We have a total ordering, so the objects with the same key are sequential.</span></span><br><span class="line">     <span class="keyword">new</span> <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] &#123;</span><br><span class="line">       <span class="comment">// 获取优先级队列 sorted</span></span><br><span class="line">       <span class="keyword">val</span> sorted = mergeSort(iterators, comparator).buffered</span><br><span class="line">       <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = sorted.hasNext</span><br><span class="line">       <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>] = &#123;</span><br><span class="line">         <span class="keyword">if</span> (!hasNext) &#123;</span><br><span class="line">           <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">NoSuchElementException</span></span><br><span class="line">         &#125;</span><br><span class="line">         <span class="comment">// 先取出一个键值对</span></span><br><span class="line">         <span class="keyword">val</span> elem = sorted.next()</span><br><span class="line">         <span class="keyword">val</span> k = elem._1</span><br><span class="line">         <span class="keyword">var</span> c = elem._2</span><br><span class="line">         <span class="comment">// 比较 k 是否相等</span></span><br><span class="line">         <span class="keyword">while</span> (sorted.hasNext &amp;&amp; sorted.head._1 == k) &#123;</span><br><span class="line">           <span class="keyword">val</span> pair = sorted.next()</span><br><span class="line">           <span class="comment">// 如果相等就合并</span></span><br><span class="line">           c = mergeCombiners(c, pair._2)</span><br><span class="line">         &#125;</span><br><span class="line">         (k, c)</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>org.apache.spark.util.collection.AppendOnlyMap</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 把key和value存在同一数组中</span></span><br><span class="line"><span class="comment">// Holds keys and values in the same array for memory locality; specifically, the order of</span></span><br><span class="line"><span class="comment">// elements is key0, value0, key1, value1, key2, value2, etc.</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">var</span> data = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">AnyRef</span>](<span class="number">2</span> * capacity)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 内存排序</span></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Return an iterator of the map in sorted order. This provides a way to sort the map without</span></span><br><span class="line"><span class="comment"> * using additional memory, at the expense of destroying the validity of the map.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">destructiveSortedIterator</span></span>(keyComparator: <span class="type">Comparator</span>[<span class="type">K</span>]): <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)] = &#123;</span><br><span class="line">  destroyed = <span class="literal">true</span></span><br><span class="line">  <span class="comment">// Pack KV pairs into the front of the underlying array</span></span><br><span class="line">  <span class="keyword">var</span> keyIndex, newIndex = <span class="number">0</span></span><br><span class="line">  <span class="comment">// 把不为null的KV对移到数组最前面</span></span><br><span class="line">  <span class="keyword">while</span> (keyIndex &lt; capacity) &#123;</span><br><span class="line">    <span class="keyword">if</span> (data(<span class="number">2</span> * keyIndex) != <span class="literal">null</span>) &#123;</span><br><span class="line">      data(<span class="number">2</span> * newIndex) = data(<span class="number">2</span> * keyIndex)</span><br><span class="line">      data(<span class="number">2</span> * newIndex + <span class="number">1</span>) = data(<span class="number">2</span> * keyIndex + <span class="number">1</span>)</span><br><span class="line">      newIndex += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    keyIndex += <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">  assert(curSize == newIndex + (<span class="keyword">if</span> (haveNullValue) <span class="number">1</span> <span class="keyword">else</span> <span class="number">0</span>))</span><br><span class="line">  <span class="comment">// 内存排序</span></span><br><span class="line">  <span class="keyword">new</span> <span class="type">Sorter</span>(<span class="keyword">new</span> <span class="type">KVArraySortDataFormat</span>[<span class="type">K</span>, <span class="type">AnyRef</span>]).sort(data, <span class="number">0</span>, newIndex, keyComparator)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">V</span>)] &#123;</span><br><span class="line">    <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">var</span> nullValueReady = haveNullValue</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = (i &lt; newIndex || nullValueReady)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): (<span class="type">K</span>, <span class="type">V</span>) = &#123;</span><br><span class="line">      <span class="keyword">if</span> (nullValueReady) &#123;</span><br><span class="line">        nullValueReady = <span class="literal">false</span></span><br><span class="line">        (<span class="literal">null</span>.asInstanceOf[<span class="type">K</span>], nullValue)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> item = (data(<span class="number">2</span> * i).asInstanceOf[<span class="type">K</span>], data(<span class="number">2</span> * i + <span class="number">1</span>).asInstanceOf[<span class="type">V</span>])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        item</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// updateFunc: 查看对应的key是否有值,如果有值,把旧值和传入的值合并;如果没值,传入的值作为初始值</span></span><br><span class="line"><span class="comment">// 更新数组中的键值对</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">changeValue</span></span>(key: <span class="type">K</span>, updateFunc: (<span class="type">Boolean</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">V</span> = &#123;</span><br><span class="line">  assert(!destroyed, destructionMessage)</span><br><span class="line">  <span class="keyword">val</span> k = key.asInstanceOf[<span class="type">AnyRef</span>]</span><br><span class="line">  <span class="keyword">if</span> (k.eq(<span class="literal">null</span>)) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!haveNullValue) &#123;</span><br><span class="line">      incrementSize()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 内存合并值</span></span><br><span class="line">    nullValue = updateFunc(haveNullValue, nullValue)</span><br><span class="line">    haveNullValue = <span class="literal">true</span></span><br><span class="line">    <span class="keyword">return</span> nullValue</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 重新hash</span></span><br><span class="line">  <span class="keyword">var</span> pos = rehash(k.hashCode) &amp; mask</span><br><span class="line">  <span class="keyword">var</span> i = <span class="number">1</span></span><br><span class="line">  <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">    <span class="comment">// 从data数组中计算出当前key</span></span><br><span class="line">    <span class="keyword">val</span> curKey = data(<span class="number">2</span> * pos)</span><br><span class="line">    <span class="keyword">if</span> (curKey.eq(<span class="literal">null</span>)) &#123;</span><br><span class="line">      <span class="comment">// 内存合并值</span></span><br><span class="line">      <span class="keyword">val</span> newValue = updateFunc(<span class="literal">false</span>, <span class="literal">null</span>.asInstanceOf[<span class="type">V</span>])</span><br><span class="line">      data(<span class="number">2</span> * pos) = k</span><br><span class="line">      data(<span class="number">2</span> * pos + <span class="number">1</span>) = newValue.asInstanceOf[<span class="type">AnyRef</span>]</span><br><span class="line">      incrementSize()</span><br><span class="line">      <span class="keyword">return</span> newValue</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (k.eq(curKey) || k.equals(curKey)) &#123;</span><br><span class="line">      <span class="comment">// 内存合并值</span></span><br><span class="line">      <span class="keyword">val</span> newValue = updateFunc(<span class="literal">true</span>, data(<span class="number">2</span> * pos + <span class="number">1</span>).asInstanceOf[<span class="type">V</span>])</span><br><span class="line">      data(<span class="number">2</span> * pos + <span class="number">1</span>) = newValue.asInstanceOf[<span class="type">AnyRef</span>]</span><br><span class="line">      <span class="keyword">return</span> newValue</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> delta = i</span><br><span class="line">      pos = (pos + delta) &amp; mask</span><br><span class="line">      i += <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="literal">null</span>.asInstanceOf[<span class="type">V</span>] <span class="comment">// Never reached but needed to keep compiler happy</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>org.apache.spark.util.collection.Spillable</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置spill阈值</span></span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> initialMemoryThreshold: <span class="type">Long</span> =</span><br><span class="line">  <span class="type">SparkEnv</span>.get.conf.getLong(<span class="string">&quot;spark.shuffle.spill.initialMemoryThreshold&quot;</span>, <span class="number">5</span> * <span class="number">1024</span> * <span class="number">1024</span>)</span><br><span class="line"><span class="comment">// spark.shuffle.spill.numElementsForceSpillThreshold</span></span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> numElementsForceSpillThreshold: <span class="type">Int</span> =</span><br><span class="line">    <span class="type">SparkEnv</span>.get.conf.get(<span class="type">SHUFFLE_SPILL_NUM_ELEMENTS_FORCE_SPILL_THRESHOLD</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">spill</span></span>(collection: <span class="type">C</span>): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spills the current in-memory collection to disk if needed. Attempts to acquire more</span></span><br><span class="line"><span class="comment"> * memory before spilling.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param collection collection to spill to disk</span></span><br><span class="line"><span class="comment"> * @param currentMemory estimated size of the collection in bytes</span></span><br><span class="line"><span class="comment"> * @return true if `collection` was spilled to disk; false otherwise</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeSpill</span></span>(collection: <span class="type">C</span>, currentMemory: <span class="type">Long</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> shouldSpill = <span class="literal">false</span></span><br><span class="line">  <span class="comment">// element个数与32取模是否等于0,判断当前内存字节数是否超过阈值</span></span><br><span class="line">  <span class="keyword">if</span> (elementsRead % <span class="number">32</span> == <span class="number">0</span> &amp;&amp; currentMemory &gt;= myMemoryThreshold) &#123;</span><br><span class="line">    <span class="comment">// Claim up to double our current memory from the shuffle memory pool</span></span><br><span class="line">    <span class="comment">// 先试着申请2倍的内存(amountToRequest 可能为负数)</span></span><br><span class="line">    <span class="keyword">val</span> amountToRequest = <span class="number">2</span> * currentMemory - myMemoryThreshold</span><br><span class="line">    <span class="comment">// 向TaskMemoryManager申请内存</span></span><br><span class="line">    <span class="comment">// TaskMemoryManager返回申请到的内存 与 请求的内存比较,如果小于请求的内存,执行如下步骤:</span></span><br><span class="line">    <span class="comment">// 1、TaskMemoryManager 先释放掉其他consumer的内存(主要是为了减少spill的频率,避免产生过多的spill文件)</span></span><br><span class="line">    <span class="comment">// 按所有consumer的使用内存进行从小到大排序,取这个map中有没有正好释放掉当前使用的内存的consumer,如果有,</span></span><br><span class="line">    <span class="comment">// 2、拿到这个符合条件的consumer,进行spill；如果没有,取使用内存最大的consumer,进行spill；循环调用,直至满足申请的内存；</span></span><br><span class="line">    <span class="comment">// 3、若调用完所有consumer后还没有足够的内存,则调用当前的consumer进行spill</span></span><br><span class="line">    <span class="keyword">val</span> granted = acquireMemory(amountToRequest)</span><br><span class="line">    myMemoryThreshold += granted</span><br><span class="line">    <span class="comment">// If we were granted too little memory to grow further (either tryToAcquire returned 0,</span></span><br><span class="line">    <span class="comment">// or we already had more memory than myMemoryThreshold), spill the current collection</span></span><br><span class="line">    <span class="comment">// 如果授予我们的内存太少而无法进一步增长（tryToAcquire返回0，或内存已经比myMemoryThreshold多），溢出当前集合</span></span><br><span class="line">    shouldSpill = currentMemory &gt;= myMemoryThreshold</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 判断条件</span></span><br><span class="line">  shouldSpill = shouldSpill || _elementsRead &gt; numElementsForceSpillThreshold</span><br><span class="line">  <span class="comment">// Actually spill</span></span><br><span class="line">  <span class="keyword">if</span> (shouldSpill) &#123;</span><br><span class="line">    _spillCount += <span class="number">1</span></span><br><span class="line">    logSpillage(currentMemory)</span><br><span class="line">    <span class="comment">// ExternalSorter.spill的实现(或 ExternalAppendOnlyMap.spill)</span></span><br><span class="line">    spill(collection)</span><br><span class="line">    _elementsRead = <span class="number">0</span></span><br><span class="line">    _memoryBytesSpilled += currentMemory</span><br><span class="line">    releaseMemory()</span><br><span class="line">  &#125;</span><br><span class="line">  shouldSpill</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/img/blog/spark-sortShuffleWriter.png" alt="spark-sortShuffleWriter.png"></p>
<h3 id="org-apache-spark-shuffle-sort-UnsafeShuffleWriter"><a href="#org-apache-spark-shuffle-sort-UnsafeShuffleWriter" class="headerlink" title="org.apache.spark.shuffle.sort.UnsafeShuffleWriter"></a>org.apache.spark.shuffle.sort.UnsafeShuffleWriter</h3><p>大致与SortShuffleWriter的原理一致,不同点在于:</p>
<pre><code>少了序列化和反序列化操作,直接对二进制数据操作
少了 反序列化-比较-序列化-溢写 逻辑
在排序数组中每条记录只占8字节,对CPU缓存更友好
</code></pre>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Nullable</span> <span class="keyword">private</span> <span class="type">ShuffleExternalSorter</span> sorter;</span><br><span class="line">  </span><br><span class="line"><span class="meta">@Override</span></span><br><span class="line">public void write(scala.collection.<span class="type">Iterator</span>&lt;<span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt;&gt; records) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  <span class="comment">// Keep track of success so we know if we encountered an exception</span></span><br><span class="line">  <span class="comment">// We do this rather than a standard try/catch/re-throw to handle</span></span><br><span class="line">  <span class="comment">// generic throwables.</span></span><br><span class="line">  boolean success = <span class="literal">false</span>;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (records.hasNext()) &#123;</span><br><span class="line">      <span class="comment">// 插入sort</span></span><br><span class="line">      insertRecordIntoSorter(records.next());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 溢出文件合并为一个文件</span></span><br><span class="line">    closeAndWriteOutput();</span><br><span class="line">    success = <span class="literal">true</span>;</span><br><span class="line">   &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (sorter != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        sorter.cleanupResources();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (<span class="type">Exception</span> e) &#123;</span><br><span class="line">        <span class="comment">// Only throw this error if we won&#x27;t be masking another</span></span><br><span class="line">        <span class="comment">// error.</span></span><br><span class="line">        <span class="keyword">if</span> (success) &#123;</span><br><span class="line">          <span class="keyword">throw</span> e;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          logger.error(<span class="string">&quot;In addition to a failure during writing, we failed during &quot;</span> +</span><br><span class="line">                       <span class="string">&quot;cleanup.&quot;</span>, e);</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@VisibleForTesting</span></span><br><span class="line">void insertRecordIntoSorter(<span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; record) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  assert(sorter != <span class="literal">null</span>);</span><br><span class="line">  <span class="keyword">final</span> <span class="type">K</span> key = record._1();</span><br><span class="line">  <span class="keyword">final</span> int partitionId = partitioner.getPartition(key);</span><br><span class="line">  serBuffer.reset();</span><br><span class="line">  serOutputStream.writeKey(key, <span class="type">OBJECT_CLASS_TAG</span>);</span><br><span class="line">  serOutputStream.writeValue(record._2(), <span class="type">OBJECT_CLASS_TAG</span>);</span><br><span class="line">  serOutputStream.flush();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">final</span> int serializedRecordSize = serBuffer.size();</span><br><span class="line">  assert (serializedRecordSize &gt; <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">  sorter.insertRecord(</span><br><span class="line">    serBuffer.getBuf(), <span class="type">Platform</span>.<span class="type">BYTE_ARRAY_OFFSET</span>, serializedRecordSize, partitionId);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@VisibleForTesting</span></span><br><span class="line">void closeAndWriteOutput() <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">  assert(sorter != <span class="literal">null</span>);</span><br><span class="line">  updatePeakMemoryUsed();</span><br><span class="line">  serBuffer = <span class="literal">null</span>;</span><br><span class="line">  serOutputStream = <span class="literal">null</span>;</span><br><span class="line">  <span class="comment">// 获取溢出文件信息</span></span><br><span class="line">  <span class="keyword">final</span> <span class="type">SpillInfo</span>[] spills = sorter.closeAndGetSpills();</span><br><span class="line">  sorter = <span class="literal">null</span>;</span><br><span class="line">  <span class="keyword">final</span> long[] partitionLengths;</span><br><span class="line">  <span class="keyword">final</span> <span class="type">File</span> output = shuffleBlockResolver.getDataFile(shuffleId, mapId);</span><br><span class="line">  <span class="keyword">final</span> <span class="type">File</span> tmp = <span class="type">Utils</span>.tempFileWith(output);</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      partitionLengths = mergeSpills(spills, tmp);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">SpillInfo</span> spill : spills) &#123;</span><br><span class="line">        <span class="keyword">if</span> (spill.file.exists() &amp;&amp; ! spill.file.delete()) &#123;</span><br><span class="line">          logger.error(<span class="string">&quot;Error while deleting spill file &#123;&#125;&quot;</span>, spill.file.getPath());</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 生成索引文件</span></span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, tmp);</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">      logger.error(<span class="string">&quot;Error while deleting temp file &#123;&#125;&quot;</span>, tmp.getAbsolutePath());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><img src="/img/blog/UnsafeWrite.png" alt="UnsafeWrite.png"></p>
<p><strong>org.apache.spark.shuffle.sort.ShuffleExternalSorter</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insertRecord</span><span class="params">(Object recordBase, <span class="keyword">long</span> recordOffset, <span class="keyword">int</span> length, <span class="keyword">int</span> partitionId)</span></span></span><br><span class="line"><span class="function">    <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// for tests</span></span><br><span class="line">    <span class="keyword">assert</span>(inMemSorter != <span class="keyword">null</span>);</span><br><span class="line">    <span class="comment">// 判断是否需要spill</span></span><br><span class="line">    <span class="keyword">if</span> (inMemSorter.numRecords() &gt;= numElementsForSpillThreshold) &#123;</span><br><span class="line">      logger.info(<span class="string">&quot;Spilling data because number of spilledRecords crossed the threshold &quot;</span> +</span><br><span class="line">        numElementsForSpillThreshold);</span><br><span class="line">      spill();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 检查是否有足够的空间将record插入到排序指针数组,如果需要额外的空间,则增加数组,如果无法获取空间,则内存中的数据将spill到磁盘</span></span><br><span class="line">    growPointerArrayIfNecessary();</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> uaoSize = UnsafeAlignedOffset.getUaoSize();</span><br><span class="line">    <span class="comment">// Need 4 or 8 bytes to store the record length.</span></span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">int</span> required = length + uaoSize;</span><br><span class="line">    <span class="comment">// 判断是否有必要申请内存,如果没有空间会spill</span></span><br><span class="line">    acquireNewPageIfNecessary(required);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(currentPage != <span class="keyword">null</span>);</span><br><span class="line">    <span class="keyword">final</span> Object base = currentPage.getBaseObject();</span><br><span class="line">    <span class="keyword">final</span> <span class="keyword">long</span> recordAddress = taskMemoryManager.encodePageNumberAndOffset(currentPage, pageCursor);</span><br><span class="line">    UnsafeAlignedOffset.putSize(base, pageCursor, length);</span><br><span class="line">    pageCursor += uaoSize;</span><br><span class="line">    Platform.copyMemory(recordBase, recordOffset, base, pageCursor, length);</span><br><span class="line">    pageCursor += length;</span><br><span class="line">    inMemSorter.insertRecord(recordAddress, partitionId);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>![tungsten-sort shuffle_write流程简图.png](/img/blog/tungsten-sort shuffle_write流程简图.png)</p>
<p><code>Resolver实现</code></p>
<ul>
<li><strong>org.apache.spark.shuffle.IndexShuffleBlockResolver</strong><br>shuffle文件对应的各个partition在文件中数据的位置</li>
</ul>
<p><code>.index</code> 文件第一位long类型为占位符,等于0</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> out = <span class="keyword">new</span> <span class="type">DataOutputStream</span>(<span class="keyword">new</span> <span class="type">BufferedOutputStream</span>(<span class="keyword">new</span> <span class="type">FileOutputStream</span>(indexTmp)))</span><br><span class="line"><span class="type">Utils</span>.tryWithSafeFinally &#123;</span><br><span class="line">  <span class="comment">// We take in lengths of each block, need to convert it to offsets.</span></span><br><span class="line">  <span class="keyword">var</span> offset = <span class="number">0</span>L</span><br><span class="line">  <span class="comment">// 第一位为占位符,0</span></span><br><span class="line">  out.writeLong(offset)</span><br><span class="line">  <span class="comment">// 遍历partition数组,lengths = 各个分区,shuffle文件的长度</span></span><br><span class="line">  <span class="keyword">for</span> (length &lt;- lengths) &#123;</span><br><span class="line"></span><br><span class="line">    offset += length</span><br><span class="line">    out.writeLong(offset)</span><br><span class="line">  &#125;</span><br><span class="line">&#125; &#123;</span><br><span class="line">  out.close()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>Reader实现</code></p>
<ul>
<li><strong>org.apache.spark.shuffle.BlockStoreShuffleReader</strong><br>reduce端去<code>mapoutputtracker</code>拿对应partition的数据,包括一次拉取的数据量大小,或者启几个线程去拿</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Read the combined key-values for this reduce task */</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">read</span></span>(): <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] = &#123;</span><br><span class="line">  <span class="comment">/** </span></span><br><span class="line"><span class="comment">    * 初始化后会调用 ShuffleBlockFetcherIterator.initialize 方法</span></span><br><span class="line"><span class="comment">    * 1、local block 与 Remote block进行分类</span></span><br><span class="line"><span class="comment">    * 2、批量发送请求远端block</span></span><br><span class="line"><span class="comment">    * 3、获取local block</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">val</span> wrappedStreams = <span class="keyword">new</span> <span class="type">ShuffleBlockFetcherIterator</span>(</span><br><span class="line">    context,</span><br><span class="line">    blockManager.shuffleClient,</span><br><span class="line">    blockManager,</span><br><span class="line">    mapOutputTracker.getMapSizesByExecutorId(handle.shuffleId, startPartition, endPartition),</span><br><span class="line">    serializerManager.wrapStream,</span><br><span class="line">    <span class="comment">// Note: we use getSizeAsMb when no suffix is provided for backwards compatibility</span></span><br><span class="line">    <span class="type">SparkEnv</span>.get.conf.getSizeAsMb(<span class="string">&quot;spark.reducer.maxSizeInFlight&quot;</span>, <span class="string">&quot;48m&quot;</span>) * <span class="number">1024</span> * <span class="number">1024</span>,</span><br><span class="line">    <span class="type">SparkEnv</span>.get.conf.getInt(<span class="string">&quot;spark.reducer.maxReqsInFlight&quot;</span>, <span class="type">Int</span>.<span class="type">MaxValue</span>),</span><br><span class="line">    <span class="type">SparkEnv</span>.get.conf.get(config.<span class="type">REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS</span>),</span><br><span class="line">    <span class="type">SparkEnv</span>.get.conf.get(config.<span class="type">MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM</span>),</span><br><span class="line">    <span class="type">SparkEnv</span>.get.conf.getBoolean(<span class="string">&quot;spark.shuffle.detectCorrupt&quot;</span>, <span class="literal">true</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> serializerInstance = dep.serializer.newInstance()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create a key/value iterator for each stream</span></span><br><span class="line">  <span class="keyword">val</span> recordIter = wrappedStreams.flatMap &#123; <span class="keyword">case</span> (blockId, wrappedStream) =&gt;</span><br><span class="line">    serializerInstance.deserializeStream(wrappedStream).asKeyValueIterator</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Update the context task metrics for each record read.</span></span><br><span class="line">  <span class="keyword">val</span> readMetrics = context.taskMetrics.createTempShuffleReadMetrics()</span><br><span class="line">  <span class="keyword">val</span> metricIter = <span class="type">CompletionIterator</span>[(<span class="type">Any</span>, <span class="type">Any</span>), <span class="type">Iterator</span>[(<span class="type">Any</span>, <span class="type">Any</span>)]](</span><br><span class="line">    recordIter.map &#123; record =&gt;</span><br><span class="line">      readMetrics.incRecordsRead(<span class="number">1</span>)</span><br><span class="line">      record</span><br><span class="line">    &#125;,</span><br><span class="line">    context.taskMetrics().mergeShuffleReadMetrics())</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> interruptibleIter = <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>[(<span class="type">Any</span>, <span class="type">Any</span>)](context, metricIter)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> aggregatedIter: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] = <span class="keyword">if</span> (dep.aggregator.isDefined) &#123;</span><br><span class="line">    <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">      <span class="comment">// 如果指定了聚合函数且允许在map端进行合并,在reduce端再次合并</span></span><br><span class="line">      <span class="keyword">val</span> combinedKeyValuesIterator = interruptibleIter.asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">C</span>)]]</span><br><span class="line">      dep.aggregator.get.combineCombinersByKey(combinedKeyValuesIterator, context)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 如果指定了聚合函数但不允许在map端进行合并，则在reduce端合并</span></span><br><span class="line">      <span class="keyword">val</span> keyValuesIterator = interruptibleIter.asInstanceOf[<span class="type">Iterator</span>[(<span class="type">K</span>, <span class="type">Nothing</span>)]]</span><br><span class="line">      dep.aggregator.get.combineValuesByKey(keyValuesIterator, context)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    interruptibleIter.asInstanceOf[<span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ExternalSorter 排序</span></span><br><span class="line">  <span class="keyword">val</span> resultIter = dep.keyOrdering <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(keyOrd: <span class="type">Ordering</span>[<span class="type">K</span>]) =&gt;</span><br><span class="line">      <span class="comment">// Create an ExternalSorter to sort the data.</span></span><br><span class="line">      <span class="keyword">val</span> sorter =</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">C</span>, <span class="type">C</span>](context, ordering = <span class="type">Some</span>(keyOrd), serializer = dep.serializer)</span><br><span class="line">      sorter.insertAll(aggregatedIter)</span><br><span class="line">      context.taskMetrics().incMemoryBytesSpilled(sorter.memoryBytesSpilled)</span><br><span class="line">      context.taskMetrics().incDiskBytesSpilled(sorter.diskBytesSpilled)</span><br><span class="line">      context.taskMetrics().incPeakExecutionMemory(sorter.peakMemoryUsedBytes)</span><br><span class="line">      <span class="comment">// Use completion callback to stop sorter if task was finished/cancelled.</span></span><br><span class="line">      context.addTaskCompletionListener[<span class="type">Unit</span>](_ =&gt; &#123;</span><br><span class="line">        sorter.stop()</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="type">CompletionIterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>], <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]]](sorter.iterator, sorter.stop())</span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">      aggregatedIter</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  resultIter <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> _: <span class="type">InterruptibleIterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]] =&gt; resultIter</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      <span class="comment">// Use another interruptible iterator here to support task cancellation as aggregator</span></span><br><span class="line">      <span class="comment">// or(and) sorter may have consumed previous interruptible iterator.</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">C</span>]](context, resultIter)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Hadoop-与-Spark-Shuffle的区别"><a href="#Hadoop-与-Spark-Shuffle的区别" class="headerlink" title="Hadoop 与 Spark Shuffle的区别"></a>Hadoop 与 Spark Shuffle的区别</h3><ul>
<li>Hadoop的有一个Map完成，Reduce便可以去fetch数据了，不必等到所有Map任务完成，而Spark的必须等到父stage完成，也就是父stage的map操作全部完成才能去fetch数据。 </li>
<li>Hadoop的Shuffle是sort-base的，那么不管是Map的输出，还是Reduce的输出，都是partion内有序的，而spark不要求这一点。</li>
<li>Hadoop的Reduce要等到fetch完全部数据，才将数据传入reduce函数进行聚合，而spark是一边fetch一边聚合。</li>
</ul>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.infoq.cn/article/swtvtetasjmytkk3ke0b">https://www.infoq.cn/article/swtvtetasjmytkk3ke0b</a></p>
<p><a target="_blank" rel="noopener" href="https://0x0fff.com/spark-architecture-shuffle/">https://0x0fff.com/spark-architecture-shuffle/</a></p>
</blockquote>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Asura7969</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://asura7969.github.io/2020/10/26/Spark%20Shuffle/">https://asura7969.github.io/2020/10/26/Spark%20Shuffle/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://asura7969.github.io" target="_blank">Asura7969 Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/spark/">spark</a></div><div class="post_share"><div class="social-share" data-image="/img/topimg/202105161054.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2020/11/01/Spark%20%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6/"><img class="prev-cover" src="/img/topimg/202105161053.png" onerror="onerror=null;src='/img/404-b.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Spark 任务调度</div></div></a></div><div class="next-post pull-right"><a href="/2020/10/18/Spark%20%E6%89%A9%E5%B1%95/"><img class="next-cover" src="/img/topimg/202105161055.png" onerror="onerror=null;src='/img/404-b.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Spark 扩展功能</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/11/01/Spark Rpc/" title="Spark Rpc"><img class="cover" src="/img/topimg/202105161051.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-11-01</div><div class="title">Spark Rpc</div></div></a></div><div><a href="/2020/11/01/Spark Sql/" title="Spark Sql"><img class="cover" src="/img/topimg/202105161052.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-11-01</div><div class="title">Spark Sql</div></div></a></div><div><a href="/2020/11/01/Spark 任务调度/" title="Spark 任务调度"><img class="cover" src="/img/topimg/202105161053.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-11-01</div><div class="title">Spark 任务调度</div></div></a></div><div><a href="/2020/09/23/Spark/" title="Spark"><img class="cover" src="/img/topimg/202105161056.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-09-23</div><div class="title">Spark</div></div></a></div><div><a href="/2020/10/18/Spark 扩展/" title="Spark 扩展功能"><img class="cover" src="/img/topimg/202105161055.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-10-18</div><div class="title">Spark 扩展功能</div></div></a></div><div><a href="/2021/04/01/SparkStreaming/" title="SparkStreaming"><img class="cover" src="/img/topimg/20210515223344.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-01</div><div class="title">SparkStreaming</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/shanyi.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">Asura7969</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">38</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">7</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Asura7969"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Asura7969" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1402357969@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-Shuffle"><span class="toc-number">1.</span> <span class="toc-text">Spark Shuffle</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">1.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#org-apache-spark-shuffle-sort-SortShuffleManager"><span class="toc-number">1.2.</span> <span class="toc-text">org.apache.spark.shuffle.sort.SortShuffleManager</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#org-apache-spark-shuffle-sort-BypassMergeSortShuffleWriter"><span class="toc-number">1.2.1.</span> <span class="toc-text">org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#org-apache-spark-shuffle-sort-SortShuffleWriter"><span class="toc-number">1.2.2.</span> <span class="toc-text">org.apache.spark.shuffle.sort.SortShuffleWriter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#org-apache-spark-shuffle-sort-UnsafeShuffleWriter"><span class="toc-number">1.2.3.</span> <span class="toc-text">org.apache.spark.shuffle.sort.UnsafeShuffleWriter</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop-%E4%B8%8E-Spark-Shuffle%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.2.4.</span> <span class="toc-text">Hadoop 与 Spark Shuffle的区别</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/12/09/Flink%201.16.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/" title="Flink 1.16.0源码编译"><img src="/img/topimg/202106050953.png" onerror="this.onerror=null;this.src='/img/404-b.jpg'" alt="Flink 1.16.0源码编译"/></a><div class="content"><a class="title" href="/2022/12/09/Flink%201.16.0%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/" title="Flink 1.16.0源码编译">Flink 1.16.0源码编译</a><time datetime="2022-12-09T14:17:57.000Z" title="发表于 2022-12-09 22:17:57">2022-12-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/15/Rust-NoteBook/" title="Rust-NoteBook"><img src="/img/topimg/rust.png" onerror="this.onerror=null;this.src='/img/404-b.jpg'" alt="Rust-NoteBook"/></a><div class="content"><a class="title" href="/2022/11/15/Rust-NoteBook/" title="Rust-NoteBook">Rust-NoteBook</a><time datetime="2022-11-15T12:24:45.000Z" title="发表于 2022-11-15 20:24:45">2022-11-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/10/13/Deltalake-CDF-CDC/" title="Deltalake CDF &amp; CDC"><img src="/img/topimg/16.png" onerror="this.onerror=null;this.src='/img/404-b.jpg'" alt="Deltalake CDF &amp; CDC"/></a><div class="content"><a class="title" href="/2022/10/13/Deltalake-CDF-CDC/" title="Deltalake CDF &amp; CDC">Deltalake CDF &amp; CDC</a><time datetime="2022-10-13T12:54:35.000Z" title="发表于 2022-10-13 20:54:35">2022-10-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/02/17/kafka%E5%8D%87%E7%BA%A7-Security/" title="kafka升级-Security"><img src="/img/topimg/202202172112.png" onerror="this.onerror=null;this.src='/img/404-b.jpg'" alt="kafka升级-Security"/></a><div class="content"><a class="title" href="/2022/02/17/kafka%E5%8D%87%E7%BA%A7-Security/" title="kafka升级-Security">kafka升级-Security</a><time datetime="2022-02-17T13:11:12.000Z" title="发表于 2022-02-17 21:11:12">2022-02-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/02/17/kafka-SCRAM-ACL%E9%85%8D%E7%BD%AE/" title="kafka &amp;&amp; SCRAM + ACL配置"><img src="/img/topimg/202111111633.png" onerror="this.onerror=null;this.src='/img/404-b.jpg'" alt="kafka &amp;&amp; SCRAM + ACL配置"/></a><div class="content"><a class="title" href="/2022/02/17/kafka-SCRAM-ACL%E9%85%8D%E7%BD%AE/" title="kafka &amp;&amp; SCRAM + ACL配置">kafka &amp;&amp; SCRAM + ACL配置</a><time datetime="2022-02-17T13:04:36.000Z" title="发表于 2022-02-17 21:04:36">2022-02-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Asura7969</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>